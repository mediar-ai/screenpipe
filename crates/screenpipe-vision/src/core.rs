#[cfg(target_os = "macos")]
use crate::apple::perform_ocr_apple;
use crate::capture_screenshot_by_window::CapturedWindow;
use crate::capture_screenshot_by_window::WindowFilters;
use crate::custom_ocr::perform_ocr_custom;
use crate::frame_comparison::{FrameComparer, FrameComparisonConfig};
#[cfg(target_os = "windows")]
use crate::microsoft::perform_ocr_windows;
use crate::monitor::get_monitor_by_id;
use crate::ocr_cache::{WindowCacheKey, WindowOcrCache};
use crate::tesseract::perform_ocr_tesseract;
use crate::utils::{capture_monitor_image, capture_windows, OcrEngine};
use anyhow::Result;
use base64::{engine::general_purpose, Engine as _};
use chrono::{DateTime, Utc};
use image::codecs::jpeg::JpegEncoder;
use image::DynamicImage;
use image::GenericImageView;
use screenpipe_core::Language;
use screenpipe_integrations::unstructured_ocr::perform_ocr_cloud;
use serde::Deserialize;
use serde::Deserializer;
use serde::Serialize;
use serde::Serializer;
use serde_json;
use std::sync::Arc;
use std::{
    collections::HashMap,
    time::{Duration, Instant, UNIX_EPOCH},
};
use tokio::sync::mpsc::Sender;
use tokio::sync::Mutex;
use tracing::{debug, error, warn};

fn serialize_image<S>(image: &Option<DynamicImage>, serializer: S) -> Result<S::Ok, S::Error>
where
    S: serde::Serializer,
{
    if let Some(image) = image {
        let mut webp_buffer = Vec::new();
        let mut cursor = std::io::Cursor::new(&mut webp_buffer);

        let mut encoder = JpegEncoder::new_with_quality(&mut cursor, 80);

        // Encode the image as WebP
        encoder
            .encode_image(image)
            .map_err(serde::ser::Error::custom)?;

        // Base64 encode the WebP data
        let base64_string = general_purpose::STANDARD.encode(webp_buffer);

        // Serialize the base64 string
        serializer.serialize_str(&base64_string)
    } else {
        serializer.serialize_none()
    }
}

fn deserialize_image<'de, D>(deserializer: D) -> Result<Option<DynamicImage>, D::Error>
where
    D: serde::Deserializer<'de>,
{
    // Deserialize the base64 string
    let base64_string: String = serde::Deserialize::deserialize(deserializer)?;

    // Check if the base64 string is empty or invalid
    if base64_string.trim().is_empty() {
        return Ok(None);
    }

    // Decode base64 to bytes
    let image_bytes = general_purpose::STANDARD
        .decode(&base64_string)
        .map_err(serde::de::Error::custom)?;

    // Create a cursor to read from the bytes
    let cursor = std::io::Cursor::new(image_bytes);

    // Decode the JPEG data back into an image
    let image = image::load(cursor, image::ImageFormat::Jpeg).map_err(serde::de::Error::custom)?;
    Ok(Some(image))
}

fn serialize_instant<S>(instant: &Instant, serializer: S) -> Result<S::Ok, S::Error>
where
    S: Serializer,
{
    let duration_since_epoch = UNIX_EPOCH.elapsed().map_err(serde::ser::Error::custom)?;
    let instant_duration = duration_since_epoch - instant.elapsed();
    let millis = instant_duration.as_millis();
    serializer.serialize_u128(millis)
}

fn deserialize_instant<'de, D>(deserializer: D) -> Result<Instant, D::Error>
where
    D: Deserializer<'de>,
{
    let millis: u128 = Deserialize::deserialize(deserializer)?;
    Ok(Instant::now() - Duration::from_millis(millis as u64))
}

pub struct CaptureResult {
    pub image: DynamicImage,
    pub frame_number: u64,
    pub timestamp: Instant,
    /// Wall-clock timestamp captured atomically with the screenshot
    pub captured_at: DateTime<Utc>,
    pub window_ocr_results: Vec<WindowOcrResult>,
}

pub struct WindowOcrResult {
    pub image: DynamicImage,
    pub window_name: String,
    pub app_name: String,
    pub text: String,
    pub text_json: Vec<HashMap<String, String>>, // Change this line
    pub focused: bool,
    pub confidence: f64,
    pub browser_url: Option<String>,
}

pub struct OcrTaskData {
    pub image: DynamicImage,
    pub window_images: Vec<CapturedWindow>,
    pub frame_number: u64,
    pub timestamp: Instant,
    /// Wall-clock timestamp captured atomically with the screenshot
    pub captured_at: DateTime<Utc>,
    pub result_tx: Sender<CaptureResult>,
}

#[derive(Debug)]
pub enum ContinuousCaptureError {
    MonitorNotFound,
    ErrorCapturingScreenshot(String),
    ErrorProcessingOcr(String),
    ErrorSendingOcrResult(String),
}

impl std::fmt::Display for ContinuousCaptureError {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        write!(f, "{:?}", self)
    }
}

/// Activity feed for adaptive FPS (optional, from screenpipe-accessibility)
#[cfg(feature = "adaptive-fps")]
pub type ActivityFeedOption = Option<screenpipe_accessibility::ActivityFeed>;

#[cfg(not(feature = "adaptive-fps"))]
pub type ActivityFeedOption = Option<()>;

pub async fn continuous_capture(
    result_tx: Sender<CaptureResult>,
    interval: Duration,
    ocr_engine: OcrEngine,
    monitor_id: u32,
    window_filters: Arc<WindowFilters>,
    languages: Vec<Language>,
    capture_unfocused_windows: bool,
    activity_feed: ActivityFeedOption,
) -> Result<(), ContinuousCaptureError> {
    let mut frame_counter: u64 = 0;
    let mut max_average: Option<MaxAverageFrame> = None;
    let mut max_avg_value = 0.0;

    // Initialize optimized frame comparer with all optimizations enabled:
    // - Hash-based early exit for identical frames (30-50% CPU reduction in static scenes)
    // - Downscaled comparison at 640x360 (60-80% faster comparisons)
    // - Single metric (histogram only, 40-50% faster than histogram+SSIM)
    let mut frame_comparer = FrameComparer::new(FrameComparisonConfig::default());

    #[cfg(feature = "adaptive-fps")]
    if activity_feed.is_some() {
        debug!("Adaptive FPS enabled - will adjust capture rate based on input activity");
    }

    // Initialize OCR cache for skipping unchanged windows
    // Cache entries expire after 5 minutes, max 100 windows cached
    let ocr_cache = Arc::new(Mutex::new(WindowOcrCache::new(
        Duration::from_secs(300),
        100,
    )));

    debug!(
        "continuous_capture: Starting using monitor: {:?}",
        monitor_id
    );
    // 1. Get monitor (mutable so we can refresh() the cached handle on failure)
    let mut monitor = match get_monitor_by_id(monitor_id).await {
        Some(m) => m,
        None => {
            error!("Monitor not found");
            return Err(ContinuousCaptureError::MonitorNotFound);
        }
    };
    let mut consecutive_capture_failures: u32 = 0;
    const MAX_CAPTURE_RETRIES: u32 = 3;
    const MAX_CONSECUTIVE_FAILURES: u32 = 30;

    // Suppress unused variable warning when feature is disabled
    #[cfg(not(feature = "adaptive-fps"))]
    let _ = activity_feed;

    loop {
        // 3. Capture monitor screenshot and wall-clock time atomically.
        //    Window capture is deferred until after frame comparison to skip
        //    expensive per-window work on unchanged frames.
        let captured_at = Utc::now();
        let (image, _capture_duration) = {
            let mut last_err = None;
            let mut captured = None;

            for attempt in 0..=MAX_CAPTURE_RETRIES {
                match capture_monitor_image(&monitor).await {
                    Ok(result) => {
                        if attempt > 0 {
                            debug!(
                                "capture succeeded after {} retries for monitor {}",
                                attempt, monitor_id
                            );
                        }
                        consecutive_capture_failures = 0;
                        captured = Some(result);
                        break;
                    }
                    Err(e) => {
                        last_err = Some(e);
                        if attempt < MAX_CAPTURE_RETRIES {
                            // Refresh the cached monitor handle â€” resolution may have
                            // changed, or the display may have been reconnected.
                            debug!(
                                "capture failed for monitor {} (attempt {}/{}), refreshing handle",
                                monitor_id,
                                attempt + 1,
                                MAX_CAPTURE_RETRIES
                            );
                            if let Err(refresh_err) = monitor.refresh().await {
                                debug!("monitor refresh failed: {}", refresh_err);
                            }
                            tokio::time::sleep(Duration::from_millis(100)).await;
                        }
                    }
                }
            }

            match captured {
                Some(result) => result,
                None => {
                    consecutive_capture_failures += 1;
                    let err = last_err.unwrap();
                    if consecutive_capture_failures >= MAX_CONSECUTIVE_FAILURES {
                        error!(
                            "monitor {} failed {} consecutive captures, bailing: {}",
                            monitor_id, consecutive_capture_failures, err
                        );
                        return Err(ContinuousCaptureError::ErrorCapturingScreenshot(
                            err.to_string(),
                        ));
                    }
                    debug!(
                        "all {} capture retries failed for monitor {} ({}/{}): {}",
                        MAX_CAPTURE_RETRIES,
                        monitor_id,
                        consecutive_capture_failures,
                        MAX_CONSECUTIVE_FAILURES,
                        err
                    );
                    tokio::time::sleep(interval).await;
                    continue;
                }
            }
        };

        // 4. Optimized frame comparison: downscales once (proportional to preserve
        //    ultrawide aspect ratios), hashes the thumbnail, then compares histograms.
        //    No full-resolution hash or redundant downscale needed.
        let current_diff = frame_comparer.compare(&image);

        // Get skip threshold from adaptive FPS or use default
        #[cfg(feature = "adaptive-fps")]
        let skip_threshold = activity_feed
            .as_ref()
            .map(|f| f.get_capture_params().skip_threshold)
            .unwrap_or(0.02);
        #[cfg(not(feature = "adaptive-fps"))]
        let skip_threshold = 0.02;

        let should_skip = current_diff < skip_threshold;

        if should_skip {
            debug!(
                "Skipping frame {} due to low difference: {:.3} < {:.3}",
                frame_counter, current_diff, skip_threshold
            );
            frame_counter += 1;
            // Use adaptive interval if enabled, otherwise use base interval
            #[cfg(feature = "adaptive-fps")]
            let sleep_interval = activity_feed
                .as_ref()
                .map(|f| f.get_capture_params().interval)
                .unwrap_or(interval);
            #[cfg(not(feature = "adaptive-fps"))]
            let sleep_interval = interval;
            tokio::time::sleep(sleep_interval).await;
            continue;
        }

        // 4b. Capture windows only for frames that passed the change threshold.
        //     This avoids expensive per-window screenshots + CGWindowList enumeration
        //     on unchanged frames (major CPU savings on multi-monitor setups).
        let window_images = capture_windows(
            &monitor,
            &window_filters,
            capture_unfocused_windows,
        ).await;

        // Track the frame with maximum difference for OCR processing
        if current_diff > max_avg_value {
            max_average = Some(MaxAverageFrame {
                image: image.clone(),
                window_images: window_images,
                image_hash: 0, // Hash is now internal to FrameComparer
                frame_number: frame_counter,
                timestamp: Instant::now(),
                captured_at,
                result_tx: result_tx.clone(),
                average: current_diff,
            });
            max_avg_value = current_diff;
        }

        // 5. Process max average frame if available
        if let Some(max_avg_frame) = max_average.take() {
            if let Err(e) = process_max_average_frame(
                max_avg_frame,
                &ocr_engine,
                languages.clone(),
                ocr_cache.clone(),
            )
            .await
            {
                error!("Error processing max average frame: {}", e);
            }
            frame_counter = 0;
            max_avg_value = 0.0;

            // Log frame comparison stats periodically
            let stats = frame_comparer.stats();
            if stats.total_comparisons.is_multiple_of(100) {
                debug!(
                    "Frame comparison stats: {} total, {} hash hits ({:.1}% hit rate)",
                    stats.total_comparisons,
                    stats.hash_hits,
                    stats.hash_hit_rate * 100.0
                );
            }
        }

        frame_counter += 1;
        // Use adaptive interval if enabled, otherwise use base interval
        #[cfg(feature = "adaptive-fps")]
        let sleep_interval = activity_feed
            .as_ref()
            .map(|f| f.get_capture_params().interval)
            .unwrap_or(interval);
        #[cfg(not(feature = "adaptive-fps"))]
        let sleep_interval = interval;
        tokio::time::sleep(sleep_interval).await;
    }
}

async fn process_max_average_frame(
    max_avg_frame: MaxAverageFrame,
    ocr_engine: &OcrEngine,
    languages: Vec<Language>,
    ocr_cache: Arc<Mutex<WindowOcrCache>>,
) -> Result<(), ContinuousCaptureError> {
    let ocr_task_data = OcrTaskData {
        image: max_avg_frame.image,
        window_images: max_avg_frame.window_images,
        frame_number: max_avg_frame.frame_number,
        timestamp: max_avg_frame.timestamp,
        captured_at: max_avg_frame.captured_at,
        result_tx: max_avg_frame.result_tx,
    };

    if let Err(e) = process_ocr_task(ocr_task_data, ocr_engine, languages, ocr_cache).await {
        error!("Error processing OCR task: {}", e);
        return Err(ContinuousCaptureError::ErrorProcessingOcr(e.to_string()));
    }

    Ok(())
}

pub struct MaxAverageFrame {
    pub image: DynamicImage,
    pub window_images: Vec<CapturedWindow>,
    pub image_hash: u64,
    pub frame_number: u64,
    pub timestamp: Instant,
    /// Wall-clock timestamp captured atomically with the screenshot
    pub captured_at: DateTime<Utc>,
    pub result_tx: Sender<CaptureResult>,
    pub average: f64,
}

pub async fn process_ocr_task(
    ocr_task_data: OcrTaskData,
    ocr_engine: &OcrEngine,
    languages: Vec<Language>,
    ocr_cache: Arc<Mutex<WindowOcrCache>>,
) -> Result<(), ContinuousCaptureError> {
    let OcrTaskData {
        image,
        window_images,
        frame_number,
        timestamp,
        captured_at,
        result_tx,
    } = ocr_task_data;

    let start_time = Instant::now();
    debug!(
        "Performing OCR for frame number since beginning of program {}",
        frame_number
    );

    let mut window_ocr_results = Vec::new();
    let mut total_confidence = 0.0;
    let mut window_count = 0;
    let mut cache_hits = 0;
    let mut cache_misses = 0;

    // Get screen dimensions for coordinate transformation
    let (screen_width, screen_height) = image.dimensions();

    for captured_window in window_images {
        // Calculate hash for this window's image
        let window_image_hash =
            WindowOcrCache::calculate_image_hash(captured_window.image.as_bytes());
        let window_id =
            WindowOcrCache::make_window_id(&captured_window.app_name, &captured_window.window_name);
        let cache_key = WindowCacheKey {
            window_id: window_id.clone(),
            image_hash: window_image_hash,
        };

        // Check cache first
        let cached_result = {
            let mut cache = ocr_cache.lock().await;
            cache.get(&cache_key)
        };

        let ocr_result = if let Some(cached) = cached_result {
            // Cache hit - reuse previous OCR result
            cache_hits += 1;
            debug!(
                "OCR cache hit for window '{}' (hash: {})",
                window_id, window_image_hash
            );

            // Still need to transform coordinates for the current position
            let parsed_json = parse_json_output(&cached.text_json);
            let transformed_json = transform_ocr_coordinates_to_screen(
                parsed_json,
                captured_window.window_x,
                captured_window.window_y,
                captured_window.window_width,
                captured_window.window_height,
                screen_width,
                screen_height,
            );

            total_confidence += cached.confidence;
            window_count += 1;

            WindowOcrResult {
                image: captured_window.image,
                window_name: captured_window.window_name,
                app_name: captured_window.app_name,
                text: cached.text.clone(),
                text_json: transformed_json,
                focused: captured_window.is_focused,
                confidence: cached.confidence,
                browser_url: captured_window.browser_url,
            }
        } else {
            // Cache miss - perform OCR
            cache_misses += 1;
            let result = process_window_ocr(
                captured_window,
                ocr_engine,
                &languages,
                &mut total_confidence,
                &mut window_count,
                screen_width,
                screen_height,
            )
            .await
            .map_err(|e| ContinuousCaptureError::ErrorProcessingOcr(e.to_string()))?;

            // Cache the result for future use (serialize JSON for storage)
            {
                let mut cache = ocr_cache.lock().await;
                let json_str = serde_json::to_string(&result.text_json).unwrap_or_default();
                cache.insert(cache_key, result.text.clone(), json_str, result.confidence);
            }

            result
        };

        window_ocr_results.push(ocr_result);
    }

    // Log cache performance
    if cache_hits > 0 || cache_misses > 0 {
        debug!(
            "OCR cache stats for frame {}: {} hits, {} misses ({:.1}% hit rate)",
            frame_number,
            cache_hits,
            cache_misses,
            if cache_hits + cache_misses > 0 {
                (cache_hits as f64 / (cache_hits + cache_misses) as f64) * 100.0
            } else {
                0.0
            }
        );
    }

    // Create and send the result
    let capture_result = CaptureResult {
        image,
        frame_number,
        timestamp,
        captured_at,
        window_ocr_results,
    };

    send_ocr_result(&result_tx, capture_result)
        .await
        .map_err(|e| ContinuousCaptureError::ErrorSendingOcrResult(e.to_string()))?;

    // Log performance metrics
    log_ocr_performance(start_time, window_count, total_confidence, frame_number);

    Ok(())
}

async fn process_window_ocr(
    captured_window: CapturedWindow,
    ocr_engine: &OcrEngine,
    languages: &[Language],
    total_confidence: &mut f64,
    window_count: &mut u32,
    screen_width: u32,
    screen_height: u32,
) -> Result<WindowOcrResult, ContinuousCaptureError> {
    // Use the browser URL that was captured atomically with the screenshot
    // This prevents timing mismatches where URL is fetched after browser navigation
    let browser_url = captured_window.browser_url.clone();

    // Perform OCR based on the selected engine
    let (window_text, window_json_output, confidence) =
        perform_ocr_with_engine(ocr_engine, &captured_window.image, languages.to_vec())
            .await
            .map_err(|e| ContinuousCaptureError::ErrorProcessingOcr(e.to_string()))?;

    // Update confidence metrics
    if let Some(conf) = confidence {
        *total_confidence += conf;
        *window_count += 1;
    }

    // Parse the OCR JSON and transform coordinates from window-relative to screen-relative
    let parsed_json = parse_json_output(&window_json_output);
    let transformed_json = transform_ocr_coordinates_to_screen(
        parsed_json,
        captured_window.window_x,
        captured_window.window_y,
        captured_window.window_width,
        captured_window.window_height,
        screen_width,
        screen_height,
    );

    Ok(WindowOcrResult {
        image: captured_window.image,
        window_name: captured_window.window_name,
        app_name: captured_window.app_name,
        text: window_text,
        text_json: transformed_json,
        focused: captured_window.is_focused,
        confidence: confidence.unwrap_or(0.0),
        browser_url,
    })
}

async fn perform_ocr_with_engine(
    ocr_engine: &OcrEngine,
    image: &DynamicImage,
    languages: Vec<Language>,
) -> Result<(String, String, Option<f64>), ContinuousCaptureError> {
    match ocr_engine {
        OcrEngine::Unstructured => perform_ocr_cloud(image, languages)
            .await
            .map_err(|e| ContinuousCaptureError::ErrorProcessingOcr(e.to_string())),
        OcrEngine::Tesseract => Ok(perform_ocr_tesseract(image, languages)),
        #[cfg(target_os = "windows")]
        OcrEngine::WindowsNative => perform_ocr_windows(image)
            .await
            .map_err(|e| ContinuousCaptureError::ErrorProcessingOcr(e.to_string())),
        #[cfg(target_os = "macos")]
        OcrEngine::AppleNative => Ok(perform_ocr_apple(image, &languages)),
        OcrEngine::Custom(config) => perform_ocr_custom(image, languages, config)
            .await
            .map_err(|e| ContinuousCaptureError::ErrorProcessingOcr(e.to_string())),
        _ => Err(ContinuousCaptureError::ErrorProcessingOcr(
            "Unsupported OCR engine".to_string(),
        )),
    }
}

async fn send_ocr_result(
    result_tx: &Sender<CaptureResult>,
    capture_result: CaptureResult,
) -> Result<(), ContinuousCaptureError> {
    // Add channel health check
    if result_tx.capacity() == 0 {
        warn!("OCR task channel at capacity - receiver may be blocked or slow");
    }

    if let Err(e) = result_tx.send(capture_result).await {
        if e.to_string().contains("channel closed") {
            error!("OCR task channel closed, recording may have stopped: {}", e);
            return Err(ContinuousCaptureError::ErrorSendingOcrResult(
                "Channel closed - recording appears to have stopped".to_string(),
            ));
        }

        error!("Failed to send OCR result: {}", e);
        return Err(ContinuousCaptureError::ErrorSendingOcrResult(format!(
            "Failed to send OCR result: {}",
            e
        )));
    }

    Ok(())
}

fn log_ocr_performance(
    start_time: Instant,
    window_count: u32,
    total_confidence: f64,
    frame_number: u64,
) {
    let duration = start_time.elapsed();
    let avg_confidence = if window_count > 0 {
        total_confidence / window_count as f64
    } else {
        0.0
    };
    debug!(
        "OCR task processed frame {} with {} windows in {:?}, average confidence: {:.2}",
        frame_number, window_count, duration, avg_confidence
    );
}

fn parse_json_output(json_output: &str) -> Vec<HashMap<String, String>> {
    let parsed_output: Vec<HashMap<String, String>> = serde_json::from_str(json_output)
        .unwrap_or_else(|e| {
            error!("Failed to parse JSON output: {}", e);
            Vec::new()
        });

    parsed_output
}

/// Transform OCR coordinates from window-relative (normalized 0-1) to screen-relative (normalized 0-1).
///
/// OCR engines return coordinates normalized to the window image dimensions.
/// This function transforms them to be normalized to the full screen dimensions,
/// which is necessary because the video frames store the full screen capture.
fn transform_ocr_coordinates_to_screen(
    ocr_blocks: Vec<HashMap<String, String>>,
    window_x: i32,
    window_y: i32,
    window_width: u32,
    window_height: u32,
    screen_width: u32,
    screen_height: u32,
) -> Vec<HashMap<String, String>> {
    // Skip transformation if dimensions are invalid
    if screen_width == 0 || screen_height == 0 || window_width == 0 || window_height == 0 {
        return ocr_blocks;
    }

    let screen_w = screen_width as f64;
    let screen_h = screen_height as f64;
    let win_x = window_x as f64;
    let win_y = window_y as f64;
    let win_w = window_width as f64;
    let win_h = window_height as f64;

    ocr_blocks
        .into_iter()
        .map(|mut block| {
            // Parse the normalized window coordinates (0-1 range)
            if let (Some(left_str), Some(top_str), Some(width_str), Some(height_str)) = (
                block.get("left").cloned(),
                block.get("top").cloned(),
                block.get("width").cloned(),
                block.get("height").cloned(),
            ) {
                if let (Ok(left), Ok(top), Ok(width), Ok(height)) = (
                    left_str.parse::<f64>(),
                    top_str.parse::<f64>(),
                    width_str.parse::<f64>(),
                    height_str.parse::<f64>(),
                ) {
                    // Transform from window-relative normalized coords to screen-relative normalized coords
                    // screen_coord = (window_offset + window_coord_normalized * window_size) / screen_size
                    let screen_left = (win_x + left * win_w) / screen_w;
                    let screen_top = (win_y + top * win_h) / screen_h;
                    let screen_width_normalized = (width * win_w) / screen_w;
                    let screen_height_normalized = (height * win_h) / screen_h;

                    // Update the block with screen-relative coordinates
                    block.insert("left".to_string(), screen_left.to_string());
                    block.insert("top".to_string(), screen_top.to_string());
                    block.insert("width".to_string(), screen_width_normalized.to_string());
                    block.insert("height".to_string(), screen_height_normalized.to_string());
                }
            }
            block
        })
        .collect()
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum RealtimeVisionEvent {
    Ocr(WindowOcr),
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct WindowOcr {
    #[serde(
        serialize_with = "serialize_image",
        deserialize_with = "deserialize_image"
    )]
    pub image: Option<DynamicImage>,
    pub window_name: String,
    pub app_name: String,
    pub text: String,
    pub text_json: Vec<HashMap<String, String>>, // Change this line
    pub focused: bool,
    pub confidence: f64,
    #[serde(
        serialize_with = "serialize_instant",
        deserialize_with = "deserialize_instant"
    )]
    pub timestamp: Instant,
    pub browser_url: Option<String>,
}
