---
title: "pipes"
description: "scheduled AI agents that run on your screen data"
icon: "wand-magic-sparkles"
---

## quick start — paste this into claude code

copy this prompt into [claude code](https://docs.anthropic.com/en/docs/claude-code), [cursor](https://cursor.com), or any AI coding assistant:

<CodeGroup>
```text create a pipe
create a screenpipe pipe that [DESCRIBE WHAT YOU WANT].

## what is screenpipe?

screenpipe is a desktop app that continuously records your screen (OCR) and audio (transcription).
it runs a local API at http://localhost:3030 that lets you query everything you've seen, said, or heard.

## what is a pipe?

a pipe is a scheduled AI agent defined as a single markdown file: ~/.screenpipe/pipes/{name}/pipe.md
every N minutes, screenpipe runs a coding agent (like pi or claude-code) with the pipe's prompt.
the agent can query your screen data, write files, call external APIs, send notifications, etc.

## pipe.md format

the file starts with YAML frontmatter, then the prompt body:

---
schedule: every 30m
enabled: true
---

Your prompt instructions here...

## context header

before execution, screenpipe prepends a context header to the prompt with:
- time range (start/end timestamps based on the schedule interval)
- current date
- user's timezone
- screenpipe API base URL
- output directory

the AI agent uses this context to query the right time range. no template variables needed in the prompt.

## screenpipe search API

the agent queries screen data via the local REST API:

curl "http://localhost:3030/search?limit=20&content_type=all&start_time=<ISO8601>&end_time=<ISO8601>"

### query parameters
- q: text search query (optional)
- content_type: "ocr" | "audio" | "ui" | "all" | "ocr+audio" | "ocr+ui" | "audio+ui"
- limit: max results (default 20)
- offset: pagination offset
- start_time / end_time: ISO 8601 timestamps
- app_name: filter by app (e.g. "chrome", "cursor")
- window_name: filter by window title
- browser_url: filter by URL (e.g. "github.com")
- min_length / max_length: filter by text length
- speaker_ids: filter audio by speaker IDs

### OCR results (what was on screen)
each result contains:
- text: the OCR'd text visible on screen
- app_name: which app was active (e.g. "Arc", "Cursor", "Slack")
- window_name: the window title
- browser_url: the URL if it was a browser
- timestamp: when it was captured
- file_path: path to the video frame
- focused: whether the window was focused

### audio results (what was said/heard)
each result contains:
- transcription: the spoken text
- speaker_id: numeric speaker identifier
- timestamp: when it was captured
- device_name: which audio device (mic or system audio)
- device_type: "input" (microphone) or "output" (system audio)

### UI events (accessibility data, macOS only)
query via: curl "http://localhost:3030/ui-events?app_name=Slack&limit=50&start_time=<ISO8601>&end_time=<ISO8601>"
event types: text (keyboard input), click, app_switch, window_focus, clipboard, scroll

## secrets

store API keys in a .env file next to pipe.md (never in the prompt itself):
echo "API_KEY=your_key" > ~/.screenpipe/pipes/my-pipe/.env
reference in prompt: source .env && curl -H "Authorization: Bearer $API_KEY" ...

## after creating the file

install: bunx screenpipe pipe install ~/.screenpipe/pipes/my-pipe
enable:  bunx screenpipe pipe enable my-pipe
test:    bunx screenpipe pipe run my-pipe
logs:    bunx screenpipe pipe logs my-pipe
```
</CodeGroup>

replace `[DESCRIBE WHAT YOU WANT]` with your use case — e.g. "tracks my time in toggl based on what apps I'm using", "writes daily summaries to obsidian", "sends me a slack message if I've been on twitter for more than 30 minutes".

---

## what are pipes?

pipes are automated workflows that run on your screenpipe data at regular intervals. each pipe is a markdown file with a prompt and a schedule. under the hood, screenpipe runs a coding agent (like [pi](https://github.com/badlogic/pi-mono)) that can query your screen data, call APIs, write files, and take actions.

**a pipe is just one file: `pipe.md`**

```
~/.screenpipe/pipes/
├── daily-journal/
│   └── pipe.md
├── toggl-sync/
│   ├── pipe.md
│   └── .env          # secrets (api keys)
└── obsidian-sync/
    └── pipe.md
```

## creating a pipe

create a folder in `~/.screenpipe/pipes/` with a `pipe.md` file:

```bash
mkdir -p ~/.screenpipe/pipes/my-pipe
cat > ~/.screenpipe/pipes/my-pipe/pipe.md << 'EOF'
---
schedule: every 30m
enabled: true
---

Summarize my screen activity for the last 30 minutes.
Query screenpipe at http://localhost:3030/search using the time range from the context header.
Write the summary to ./output/<date>.md
EOF

# install and enable
bunx screenpipe pipe install ~/.screenpipe/pipes/my-pipe
bunx screenpipe pipe enable my-pipe

# test it
bunx screenpipe pipe run my-pipe
```

## pipe.md format

every pipe.md starts with YAML frontmatter between `---` markers, followed by the prompt:

```markdown
---
schedule: every 2h
enabled: true
---

Your prompt goes here. This is what the AI agent will execute.
You can reference screenpipe's API, write files, call external APIs, etc.
```

### frontmatter fields

| field | required | default | description |
|-------|----------|---------|-------------|
| `schedule` | yes | `manual` | `every 30m`, `every 2h`, `daily`, cron (`0 */2 * * *`), or `manual` |
| `enabled` | no | `true` | whether the scheduler runs this pipe |
| `model` | no | `claude-haiku-4-5@20251001` | which LLM the agent uses |
| `provider` | no | `screenpipe` | LLM provider: `screenpipe` (cloud), `anthropic`, `openai`, `google`, etc. |
| `preset` | no | — | AI preset id from app settings (overrides model/provider) |

### context header

before execution, screenpipe prepends a context header to the prompt:

```
Time range: 2026-02-12T13:00:00Z to 2026-02-12T14:00:00Z
Date: 2026-02-12
Timezone: PST (UTC-08:00)
Output directory: ./output/
Screenpipe API: http://localhost:3030
```

the AI agent uses these values to query the right time range and format output correctly. no template variables needed — just write plain instructions.

### schedule formats

| format | example | description |
|--------|---------|-------------|
| interval | `every 30m`, `every 2h` | runs at fixed intervals |
| daily | `daily` | runs once per day |
| cron | `0 */2 * * *` | standard 5-field cron expression |
| manual | `manual` | only runs when triggered manually |

## cli commands

```bash
# list all pipes
bunx screenpipe pipe list

# install a pipe from file, folder, or URL
bunx screenpipe pipe install ./my-pipe.md
bunx screenpipe pipe install https://raw.githubusercontent.com/user/repo/main/pipe.md

# enable/disable
bunx screenpipe pipe enable my-pipe
bunx screenpipe pipe disable my-pipe

# run once manually
bunx screenpipe pipe run my-pipe

# view run history
bunx screenpipe pipe logs my-pipe
bunx screenpipe pipe logs my-pipe -f  # follow

# delete
bunx screenpipe pipe delete my-pipe
```

## http api

when screenpipe is running, pipes are also manageable via the local API:

```bash
# list all pipes
curl http://localhost:3030/pipes

# run a pipe
curl -X POST http://localhost:3030/pipes/my-pipe/run

# enable/disable
curl -X POST http://localhost:3030/pipes/my-pipe/enable \
  -H "Content-Type: application/json" \
  -d '{"enabled": true}'

# update pipe content
curl -X POST http://localhost:3030/pipes/my-pipe/config \
  -H "Content-Type: application/json" \
  -d '{"raw_content": "---\nschedule: every 1h\nenabled: true\n---\n\nYour prompt here..."}'

# view logs
curl http://localhost:3030/pipes/my-pipe/logs

# install from URL
curl -X POST http://localhost:3030/pipes/install \
  -H "Content-Type: application/json" \
  -d '{"source": "https://example.com/pipe.md"}'
```

## app ui

go to **settings → pipes** to see all installed pipes, toggle them on/off, run them manually, edit the pipe.md directly, select an AI preset, and view logs.

## examples

### time tracking (toggl)

```markdown
---
schedule: every 1m
enabled: true
---

Automatically update my Toggl time tracking based on screen activity.

1. Query screenpipe search API for the time range in the context header
2. Read API key: source .env
3. Check current Toggl timer
4. If activity changed, stop old timer and start new one

Activity rules:
- VSCode/Cursor/Terminal → "coding"
- Chrome with GitHub → "code review"
- Slack/Discord → "communication"
- Zoom/Meet → "meeting"
```

### daily journal (obsidian)

```markdown
---
schedule: every 2h
enabled: true
---

Summarize my screen activity into a daily journal entry.
Query screenpipe search API for the time range in the context header.
Write to ~/obsidian-vault/screenpipe/<date>.md
Use [[wiki-links]] for people and projects.
Include timeline deep links: [time](screenpipe://timeline?timestamp=<ISO8601>)
```

### standup report

```markdown
---
schedule: daily
enabled: true
---

Generate a standup report from yesterday's screen activity.
Format: what I did, what I'm doing, blockers.
Write to ./output/<date>.md
```

## AI presets

in the screenpipe app, go to **settings → AI settings** to configure presets (model + provider combinations). in **settings → pipes**, you can assign a preset to each pipe — this overrides the model/provider in the frontmatter.

for CLI users, screenpipe auto-creates a default preset using screenpipe cloud.

## AI providers

by default, pipes use **screenpipe cloud** — no setup needed if you have a screenpipe account.

to use your own AI subscription (Claude Pro, ChatGPT Plus, Gemini, or API keys), pipes reuse [pi's native auth system](https://github.com/badlogic/pi-mono):

### option 1: subscription (free with existing plan)

```bash
# run pi interactively and use /login
pi
# then type: /login
# select Claude Pro, ChatGPT Plus, GitHub Copilot, or Google Gemini
```

### option 2: API key

add to `~/.pi/agent/auth.json`:

```json
{
  "anthropic": { "type": "api_key", "key": "sk-ant-..." },
  "openai": { "type": "api_key", "key": "sk-..." },
  "google": { "type": "api_key", "key": "..." }
}
```

or set environment variables: `ANTHROPIC_API_KEY`, `OPENAI_API_KEY`, `GEMINI_API_KEY`.

### using in a pipe

add `provider` to your pipe.md frontmatter:

```yaml
---
schedule: every 30m
provider: anthropic
model: claude-haiku-4-5@20251001
---
```

**provider resolution:** preset (if set) → frontmatter provider/model → screenpipe cloud.

## secrets

store API keys in `.env` files inside the pipe folder:

```bash
echo "TOGGL_API_KEY=your_key_here" > ~/.screenpipe/pipes/toggl-sync/.env
```

the pipe prompt can reference them: `source .env && curl -u $TOGGL_API_KEY:api_token ...`

**never put secrets in pipe.md** — the prompt may be visible in logs.

## architecture

```
pipe.md (prompt + config)
  → pipe manager (parses frontmatter, schedules runs)
    → agent executor (pi, claude-code, etc.)
      → agent queries screenpipe API + executes actions
        → output saved to pipe folder
```

- **agent ≠ model**: the agent is the CLI tool (pi, claude-code). the model is the LLM (haiku, opus, llama).
- **one pipe runs at a time** (global semaphore prevents overlap)
- **lookback = schedule interval** (capped at 8h to prevent context overflow)
- **logs saved** to `~/.screenpipe/pipes/{name}/logs/` as JSON

## built-in pipes

screenpipe ships with three template pipes (disabled by default):

- **obsidian-sync** — sync screen activity to obsidian vault as daily logs
- **reminders** — scan activity for todos and create apple reminders (macOS)
- **idea-tracker** — surface startup ideas from your browsing + market trends
