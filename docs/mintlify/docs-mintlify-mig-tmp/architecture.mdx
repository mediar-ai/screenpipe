---
title: "architecture"
sidebarTitle: "architecture"
description: "How screenpipe captures screens, processes OCR, stores data locally in SQLite, and provides APIs for AI."
icon: "sitemap"
---

## overview

screenpipe is a Rust application that continuously captures your screen and audio, processes them locally, and stores everything in a SQLite database. it serves a REST API on `localhost:3030` for querying your data.

```
┌─────────────┐     ┌──────────────┐     ┌──────────┐     ┌───────────┐
│   capture    │ ──▸ │  processing  │ ──▸ │  storage │ ──▸ │    API    │
│ screen/audio │     │  OCR / STT   │     │  SQLite  │     │ :3030     │
└─────────────┘     └──────────────┘     └──────────┘     └───────────┘
                                                                │
                                                          ┌─────┴─────┐
                                                          │   pipes   │
                                                          │ AI agents │
                                                          └───────────┘
```

## layers

### 1. capture

- **screen**: captures frames at configurable FPS (default 1.0, macOS 0.5)
- **audio**: captures from multiple input/output devices in configurable chunks (default 30s)
- **UI events**: keyboard, mouse, app switches, clipboard via accessibility APIs (macOS)

### 2. processing

- **OCR**: extracts text from frames — engines: `apple-native`, `windows-native`, `tesseract`, `unstructured`, `custom`
- **STT**: speech-to-text — engines: `whisper-large-v3-turbo`, `whisper-tiny`, `deepgram`
- **speaker ID**: identifies and labels different speakers
- **PII removal**: optional redaction of sensitive text

### 3. storage

- **SQLite database** at `~/.screenpipe/db.sqlite` — metadata, OCR text, transcriptions, tags
- **media files** at `~/.screenpipe/data/` — MP4 screen recordings, audio chunks
- all data stays local on your machine

### 4. API

REST API on `localhost:3030`:

- `/search` — filtered content retrieval
- `/frames/{id}` — access captured frames
- `/health` — system status
- `/raw_sql` — direct database queries
- see [CLI reference](/cli-reference) for full endpoint list

### 5. pipes

[pipes](/pipes) are scheduled AI agents (`.md` files) that run on your screen data. an AI agent (like [pi](https://github.com/badlogic/pi-mono)) reads the prompt, queries the screenpipe API, and takes action.

pipes run in `~/.screenpipe/pipes/{name}/` and are scheduled via cron-like intervals.

## data flow

1. **capture** → screen frames + audio chunks recorded to disk
2. **process** → OCR extracts text from frames, STT transcribes audio
3. **store** → processed data indexed in SQLite, media files on disk
4. **query** → apps/pipes search via REST API on localhost:3030
5. **act** → pipes use AI to analyze data and take actions (write notes, track time, send reminders)

## database schema

key tables:

- `frames` — captured screen frame metadata
- `ocr_results` — text extracted from frames
- `audio_chunks` — audio recording metadata
- `transcriptions` — text from audio
- `speakers` — identified speakers
- `tags` — user-applied tags on content

inspect directly:

```bash
sqlite3 ~/.screenpipe/db.sqlite .schema
```

## resource usage

runs 24/7 on a MacBook Pro M3 (32GB) or a $400 Windows laptop:

- ~600 MB RAM
- ~10% CPU
- ~30 GB/month storage at 1 FPS

## source code

- [server.rs](https://github.com/screenpipe/screenpipe/blob/main/crates/screenpipe-server/src/server.rs) — API server
- [core.rs](https://github.com/screenpipe/screenpipe/blob/main/crates/screenpipe-server/src/core.rs) — recording orchestration
- [vision core.rs](https://github.com/screenpipe/screenpipe/blob/main/crates/screenpipe-vision/src/core.rs) — screen capture
- [pipes/mod.rs](https://github.com/screenpipe/screenpipe/blob/main/crates/screenpipe-core/src/pipes/mod.rs) — pipe manager
