---
title: "build plugins for screenpipe - extend with TypeScript & Next.js"
sidebarTitle: "plugins (pipes)"
description: "Build plugins (pipes) for screenpipe. Use TypeScript and Next.js to create apps that process screen recordings, generate summaries, and integrate with AI."
icon: "puzzle-piece"
---

<div style="display: flex; justify-content: center; margin-bottom: 2rem;">
  <dotlottie-player
    src="/animations/pipes-flow.json"
    background="transparent"
    speed="1"
    style="width: 350px; height: 140px;"
    loop
    autoplay
  ></dotlottie-player>
</div>

screenpipe is built for extensibility through plugins that interact with captured screen and audio data. whether you need to tag activities, generate summaries, or send data to third-party services, plugins let you build powerful workflows.

plugins run within screenpipe's sandboxed environment. written in typescript/javascript and nextjs.

[feel free to use our docs as context in cursor agent through mcp](https://docs.screenpi.pe/mcp-server#mintlify-mcp-use-our-docs-in-cursor%2C-claude%2C-etc)

### why build pipes?

regardless of progress in ai architecture, it's as good as the given context. screenpipe is the bridge between dull hallucinating ai and super intelligent agents.

#### for developers

- **zero infrastructure**: 100% local by default, no servers or complex setups, access to your auth tokens (unlike cloud agents)
- **typescript \+ rust \+ bun**: blazing fast environment, highly optimized pipeline, 4 ai models, running on $200 laptops, best local stt in the market
- **full context**: rich ocr, desktop scrapping, keyboard/mouse, and audio transcription apis
- **open source**: no bs, no tricks, mit license
- **standalone apps**: export your pipes as standalone desktop apps
  - [screenpipe-tauri-template](https://github.com/LorenzoBloedow/screenpipe-tauri-template-dev)
  - [screenpipe-electron-template](https://github.com/neo773/screenpipe-electron)

#### killer features

- **ai flexibility**: openai, local llms (ollama), anthropic, gemini, etc.
- **rich apis**:
  - `pipe.queryScreenpipe` for context
  - `pipe.settings` for app settings
  - experimental `pipe.streamTranscriptions` for audio transcription streaming (make sure to enable it in settings)
  - experimental `pipe.streamVision` for ocr/accessibility/frames/browser url streaming
  - experimental `pipe.input` for keyboard/mouse control
- **sandboxed & cross-platform**: safe execution on all os
- **real-time**: process screen & audio as it happens
- **cron jobs**: schedule your pipes to run at specific times, same api as vercel
- **nextjs**: build desktop native apps with nextjs - no native hell

### quick start

we recommend using `bun`, [install](https://bun.sh/docs/installation) or make sure it's up to date.

the fastest way to create a new pipe is using our cli:

follow installation instructions & test your pipe locally

```bash
bunx --bun @screenpipe/dev@latest pipe create # or use npx @screenpipe/dev pipe create
bun dev
```

### developer cli

the developer cli provides tools for building and managing pipes:

```bash
# add predefined components to your pipe (shadcn style)
bunx --bun @screenpipe/dev@latest components add
# select components from the interactive menu:
# - use-health: health monitoring hooks
# - use-settings: settings management
# - route-settings: settings page routing
# - use-sql-autocomplete: sql query assistance
# - sql-autocomplete-input: sql input component
# - use-search-history: search history management
# - use-ai-provider: ai integration hooks
```

### installing pipes

you can install pipes in the screenpipe app:

```bash
# install from local path or GitHub URL
screenpipe pipe install <path-or-url>

# enable the pipe
screenpipe pipe enable <pipe-id>

# list installed pipes
screenpipe pipe list
```

or use the UI in the screenpipe desktop app to install pipes by URL.

### available pipes

| |                                                                        | |
| --- | ---------------------------------------------------------------------- | --- |
| | automate your second brain by logging activities to obsidian           | |
| | google-photo like gallery of your screen recordings with ai insights   | |
| | explore your data in a powerful table view with filtering and sorting  | |
| | search through your screen recordings and audio transcripts with ai    | |
| | visualize your day with ai-powered timeline of activities              | |
| | automatically identify and label different speakers using ai           | |
| | organize and summarize meetings with ai - get transcripts and insights | |
| | automate development on linkedin                              | |

to install a pipe, add the GitHub URL in the screenpipe app and click install.

### llm links

paste these links into your cursor chat for context:

- https://github.com/mediar-ai/screenpipe/blob/main/screenpipe-js/browser-sdk/src/index.ts
- https://github.com/mediar-ai/screenpipe/blob/main/pipes/obsidian/src/app/api/log/route.ts
- https://github.com/mediar-ai/screenpipe/blob/main/pipes/search/src/components/search-chat.tsx
- https://github.com/mediar-ai/screenpipe/blob/main/pipes/rewind/src/lib/hooks/use-app-name-suggestion.tsx
- https://github.com/mediar-ai/screenpipe/blob/main/screenpipe-server/src/server.rs
- https://github.com/mediar-ai/screenpipe/blob/main/screenpipe-core/src/pipes.rs
